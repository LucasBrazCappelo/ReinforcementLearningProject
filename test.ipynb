{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork(nn.Module):\n",
    "    def __init__(self, input_size=18, hidden_size1=128, hidden_size2=128, output_size=9):\n",
    "        super(Qnetwork, self).__init__()\n",
    "        self.flattener = nn.Flatten()\n",
    "        self.inputLayer = nn.Linear(input_size, hidden_size1)\n",
    "        self.fullyConnected = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.outputLayer = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flattener(x)\n",
    "        x = F.relu(self.inputLayer(x))\n",
    "        x = F.relu(self.fullyConnected(x))\n",
    "        x = self.outputLayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,epsilon=0.2,player='X', learningRate=0.05, discountFactor=0.99, n_max=100):\n",
    "        if isinstance(epsilon, tuple):\n",
    "            self.epsilon_min, self.epsilon_max = epsilon\n",
    "            self.epsilon = self.epsilon_max\n",
    "        else:\n",
    "            self.epsilon = epsilon\n",
    "            self.epsilon_min = epsilon\n",
    "            self.epsilon_max = epsilon\n",
    "        self.learningRate = learningRate\n",
    "        self.discountFactor = discountFactor\n",
    "\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "\n",
    "        self.n = 0\n",
    "        self.n_max = n_max\n",
    "\n",
    "        self.isLearning = True\n",
    "\n",
    "        self.player = player # 'X' or 'O'\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_max * (1 - self.n / self.n_max))\n",
    "\n",
    "    def set_player(self, player = 'X', j=-1):\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def empty(self, state):\n",
    "        \"\"\" Return all empty positions. \"\"\"\n",
    "        availableActions = []\n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                position = (x, y)\n",
    "                if state[position] == 0:\n",
    "                    availableActions.append(position)\n",
    "        return availableActions\n",
    "\n",
    "    def randomAction(self, state):\n",
    "        \"\"\" Choose a random action from the available options. \"\"\"\n",
    "        availableActions = self.empty(state)\n",
    "\n",
    "        return random.choice(availableActions)\n",
    "    \n",
    "    def bestAction(self, state):\n",
    "        pass\n",
    "\n",
    "    def act(self,state):\n",
    "        pass\n",
    "\n",
    "    def learn(self, s_prime, reward, end=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent(Agent):\n",
    "    \"\"\"\n",
    "    Our Q-network will be a simple linear neural network with two hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=0.2, player='X', learningRate=0.0005, discountFactor=1.0 , n_max=100, Qmodel=Qnetwork(), batch_size=64, C=500 , R=deque(maxlen=10_000), criterion=nn.HuberLoss()):\n",
    "        super(DQN_agent, self).__init__()\n",
    "\n",
    "        # If a GPU is available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.Qmodel = Qmodel.to(self.device)\n",
    "        self.Qtarget = Qnetwork().to(self.device)\n",
    "        self.Qtarget.load_state_dict(self.Qmodel.state_dict())\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.t = 0\n",
    "        self.C = C\n",
    "\n",
    "        self.R = R\n",
    "\n",
    "        # criterion is Huber loss (with delta = 1)\n",
    "        self.criterion = criterion\n",
    "\n",
    "        # optimizer is Adam\n",
    "        self.optimizer = torch.optim.Adam(self.Qmodel.parameters(), lr=learningRate)        \n",
    "\n",
    "    def bestAction(self, state):\n",
    "        \"\"\"\n",
    "        Choose the available actions which have a maximum expected future reward\n",
    "        using the Q-network.\n",
    "        \"\"\"\n",
    "        # convert state to tensor, adding batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.Qmodel.forward(state)\n",
    "        return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def act(self, grid):\n",
    "        \"\"\"\n",
    "        epsilon-greedy action selection, according to the Q-table.\n",
    "        \"\"\"\n",
    "        state = torch.tensor(grid, dtype=torch.int64)\n",
    "        state = F.one_hot(state+1)\n",
    "        state = state[:,:,(2,0)]\n",
    "        state = state.unsqueeze(0)\n",
    "        state = state.type(torch.float).to(self.device)\n",
    "        self.state = state\n",
    "\n",
    "        # whether move in random or not\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.randomAction(grid)\n",
    "            self.action = action[0] * 3 + action[1]\n",
    "        else:\n",
    "            # Get the best move\n",
    "            self.action = self.bestAction(self.state)\n",
    "            # action is a tuple of (x, y) from self.action\n",
    "            action = (self.action // 3, self.action % 3)\n",
    "\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, grid, reward, end=False):\n",
    "        if self.isLearning:\n",
    "            if not end:\n",
    "                s_prime = torch.tensor(grid, dtype=torch.int64)\n",
    "                s_prime = F.one_hot(s_prime+1)\n",
    "                s_prime = s_prime[:,:,(2,0)]\n",
    "                s_prime = s_prime.unsqueeze(0)\n",
    "                s_prime = s_prime.type(torch.float).to(self.device)\n",
    "\n",
    "                self.R.append((self.state, self.action, reward, s_prime))\n",
    "            else:\n",
    "                self.R.append((self.state, self.action, reward, None))\n",
    "\n",
    "                self.state = None\n",
    "                self.action = None\n",
    "\n",
    "                self.n += 1\n",
    "                self.decrease_epsilon()\n",
    "            # self.R is a deque with maxlen=buffer_size so it auto pop\n",
    "\n",
    "            if len(self.R) < self.batch_size:\n",
    "                batch = self.R\n",
    "                # return # check if this is correct.\n",
    "            else:\n",
    "                # sample random minibatch from self.R\n",
    "                batch = random.sample(self.R, self.batch_size)\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            # convert to tensor\n",
    "            states = torch.cat([x[0] for x in batch]).to(self.device)\n",
    "            actions = [(i,x[1]) for i,x in enumerate(batch)]\n",
    "            rewards = torch.tensor([x[2] for x in batch]).to(self.device)\n",
    "            s_primes = torch.cat([x[3] for x in batch if x[3] is not None]).to(self.device)\n",
    "            s_prime_mask = torch.tensor([x[3] is not None for x in batch], device=self.device, dtype=torch.bool)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            Q_theta_sj_aj = self.Qmodel.forward(states)[actions]\n",
    "            maxQtarget = torch.zeros(self.batch_size, device=self.device)\n",
    "            maxQtarget[s_prime_mask] = self.Qtarget.forward(s_primes).max(dim=1).values.detach()\n",
    "\n",
    "            loss = self.criterion(Q_theta_sj_aj, rewards + self.discountFactor*maxQtarget)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.t += 1\n",
    "            if self.t == self.C:\n",
    "                self.t = 0\n",
    "                self.Qtarget.load_state_dict(self.Qmodel.state_dict())\n",
    "        \n",
    "        elif end:\n",
    "            self.state = None\n",
    "            self.action = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(player_opt, agent, maxGames=20_000, env=TictactoeEnv()):\n",
    "    Turns = np.array(['X','O'])\n",
    "    winnerList = np.zeros(maxGames)\n",
    "\n",
    "    pBar = trange(maxGames)\n",
    "    for nbGames in range(maxGames):\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "\n",
    "        player_opt.player = Turns[nbGames%2]\n",
    "        agent.player = Turns[(nbGames+1)%2]\n",
    "\n",
    "        for roundGame in range(9):\n",
    "            if env.current_player == player_opt.player:\n",
    "                if roundGame > 1 and isinstance(player_opt, Agent):\n",
    "                    player_opt.learn(grid, 0)\n",
    "                move = player_opt.act(grid)   \n",
    "                badMove = env.grid[move] != 0\n",
    "                if badMove:\n",
    "                    if not player_opt.isLearning:\n",
    "                        print(\"A player should continue to learn before playing for real.\")\n",
    "                        break\n",
    "                while badMove:\n",
    "                    player_opt.learn(grid, -1)\n",
    "                    move = player_opt.act(grid)\n",
    "                    badMove = env.grid[move] != 0\n",
    "            else:\n",
    "                if roundGame > 1 and isinstance(agent, Agent):\n",
    "                    agent.learn(grid, 0)\n",
    "                move = agent.act(grid)   \n",
    "                badMove = env.grid[move] != 0\n",
    "                if badMove:\n",
    "                    if not agent.isLearning:\n",
    "                        print(\"A player should continue to learn before playing for real.\")\n",
    "                        break\n",
    "                while badMove:\n",
    "                    agent.learn(grid, -1)\n",
    "                    move = agent.act(grid)\n",
    "                    badMove = env.grid[move] != 0\n",
    "                             \n",
    "\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "            if end:\n",
    "                if winner == agent.player:\n",
    "                    winnerList[nbGames] = 1\n",
    "                    if isinstance(player_opt, Agent):\n",
    "                        player_opt.learn(grid, -1, end=True)\n",
    "                    if isinstance(agent, Agent):\n",
    "                        agent.learn(grid, 1, end=True)\n",
    "                elif winner == player_opt.player:\n",
    "                    winnerList[nbGames] = -1\n",
    "                    if isinstance(player_opt, Agent):\n",
    "                        player_opt.learn(grid, 1, end=True)\n",
    "                    if isinstance(agent, Agent):\n",
    "                        agent.learn(grid, -1, end=True)\n",
    "                else:\n",
    "                    if isinstance(player_opt, Agent):\n",
    "                        player_opt.learn(grid, 0, end=True)\n",
    "                    if isinstance(agent, Agent):\n",
    "                        agent.learn(grid, 0, end=True)\n",
    "                break     \n",
    "        pBar.update(1)\n",
    "    pBar.close()\n",
    "    env.reset()\n",
    "    return winnerList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4774fe3e2548e4acb691fc24ee079c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-df4fe5287d5b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwinnerList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_games\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer_opt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxGames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20_000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mgroupSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-f2e0bf5f1b5f>\u001b[0m in \u001b[0;36mplay_games\u001b[1;34m(player_opt, agent, maxGames, env)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mroundGame\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[0mmove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mbadMove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-e09ceb5ab303>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, grid, reward, end)\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[0mQ_theta_sj_aj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0mmaxQtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             \u001b[0mmaxQtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms_prime_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_primes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_theta_sj_aj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscountFactor\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmaxQtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "epsilon = 0.6\n",
    "\n",
    "player_opt = OptimalPlayer(epsilon=0.5)\n",
    "agent = DQN_agent(epsilon=epsilon)\n",
    "\n",
    "winnerList = play_games(player_opt, agent, maxGames=20_000)\n",
    "\n",
    "groupSize = 250\n",
    "y=winnerList.reshape(winnerList.size//groupSize, groupSize).mean(axis=1)\n",
    "x=np.arange(y.size)*groupSize\n",
    "\n",
    "fig = px.line(x=x, y=y, title=f'Average reward over time of RL agent with policy epsilon={epsilon}')\n",
    "fig.update_layout(xaxis_title='Game number', yaxis_title='Average reward')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
