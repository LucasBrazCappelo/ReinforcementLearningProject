{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a GPU is available\n",
    "if not torch.cuda.is_available():\n",
    "  raise Exception(\"Things will go much quicker if you use a GPU\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([5]),\n",
       "indices=tensor([5]))"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(2,3).unsqueeze(0)\n",
    "y = nn.Flatten()(x).max(1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [(\"a\",\"b\",\"c\",torch.arange(6).reshape(2,3)),(\"a\",\"b\",\"c\",torch.arange(6).reshape(2,3)*7),(\"a\",\"b\",\"c\",None),(\"a\",\"b\",\"c\",torch.arange(6).reshape(2,3)*99)]\n",
    "\n",
    "L.extend(L[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Our Q-network will be a simple linear neural network with two hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=0.2, player='X', learningRate=0.0005, discountFactor=1.0, batch_size=64, C=500 ,n_max=100, R=deque(maxlen=10_000)):\n",
    "        super(Qnetwork, self).__init__()\n",
    "        self.flattener = nn.Flatten()\n",
    "        self.inputLayer = nn.Linear(3*3*2, 128),\n",
    "        self.fullyConnected = nn.Linear(128, 128),\n",
    "        self.outputLayer = nn.Linear(128, 9)\n",
    "\n",
    "        if isinstance(epsilon, tuple):\n",
    "            self.epsilon_min, self.epsilon_max = epsilon\n",
    "            self.epsilon = self.epsilon_max\n",
    "        else:\n",
    "            self.epsilon = epsilon\n",
    "            self.epsilon_min = epsilon\n",
    "            self.epsilon_max = epsilon\n",
    "        self.discountFactor = discountFactor\n",
    "\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "        \n",
    "        self.n = 0\n",
    "        self.n_max = n_max\n",
    "\n",
    "        self.isLearning = True\n",
    "\n",
    "        self.player = player # 'X' or 'O'\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.t = 0\n",
    "        self.C = C\n",
    "\n",
    "        self.R = R\n",
    "\n",
    "        # criterion is Huber loss (with delta = 1)\n",
    "        self.criterion = nn.HuberLoss()\n",
    "\n",
    "        # optimizer is Adam\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learningRate)\n",
    "\n",
    "        # If a GPU is available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.Qtarget = Qnetwork(epsilon, player, learningRate, discountFactor, batch_size, C, n_max, R).to(self.device) # TODO : check this at the end of modifications to see if still correct\n",
    "        self.Qtarget.load_state_dict(self.state_dict())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(self.device)\n",
    "        x = self.flattener(x)\n",
    "        x = F.relu(self.inputLayer(x))\n",
    "        x = F.relu(self.fullyConnected(x))\n",
    "        x = self.outputLayer(x)\n",
    "        return x\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_max * (1 - self.n / self.n_max))\n",
    "\n",
    "    def set_player(self, player = 'X', j=-1):\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def empty(self, state):\n",
    "        \"\"\" Return all empty positions. \"\"\"\n",
    "        availableActions = []\n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                position = (x, y)\n",
    "                if state[position] == 0:\n",
    "                    availableActions.append(position)\n",
    "        return availableActions\n",
    "\n",
    "    def randomAction(self, state):\n",
    "        \"\"\" Choose a random action from the available options. \"\"\"\n",
    "        availableActions = self.empty(state)\n",
    "\n",
    "        return random.choice(availableActions)\n",
    "\n",
    "    def bestAction(self, state):\n",
    "        \"\"\"\n",
    "        Choose the available actions which have a maximum expected future reward\n",
    "        using the Q-network.\n",
    "        \"\"\"\n",
    "        # convert state to tensor, adding batch dimension\n",
    "        with torch.no_grad:\n",
    "            q_values = self.forward(state)\n",
    "        return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def act(self, grid):\n",
    "        \"\"\"\n",
    "        epsilon-greedy action selection, according to the Q-table.\n",
    "        \"\"\"\n",
    "        state = torch.tensor(grid, dtype=torch.int64)\n",
    "        state = F.one_hot(state+1)\n",
    "        state = state[:,:,(2,0)]\n",
    "        state = state.unsqueeze(0)\n",
    "        state = state.type(torch.float).to(self.device)\n",
    "        self.state = state\n",
    "\n",
    "        # whether move in random or not\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.randomAction(grid)\n",
    "            self.action = action[0] * 3 + action[1]\n",
    "        else:\n",
    "            # Get the best move\n",
    "            self.action = self.bestAction(self.state)\n",
    "            # action is a tuple of (x, y) from self.action\n",
    "            action = (self.action // 3, self.action % 3)\n",
    "\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, grid, reward, end=False):\n",
    "        if self.isLearning:\n",
    "            if not end:\n",
    "                s_prime = torch.tensor(grid, dtype=torch.int64)\n",
    "                s_prime = F.one_hot(s_prime+1)\n",
    "                s_prime = s_prime[:,:,(2,0)]\n",
    "                s_prime = s_prime.unsqueeze(0)\n",
    "                s_prime = s_prime.type(torch.float).to(self.device)\n",
    "\n",
    "                self.R.append((self.state, self.action, reward, s_prime))\n",
    "            else:\n",
    "                self.R.append((self.state, self.action, reward, None))\n",
    "\n",
    "                self.state = None\n",
    "                self.action = None\n",
    "\n",
    "                self.n += 1\n",
    "                self.decrease_epsilon()\n",
    "            # self.R is a deque with maxlen=buffer_size so it auto pop\n",
    "\n",
    "            if len(self.R) < self.batch_size:\n",
    "                pass\n",
    "                # return # TODO: check if this is correct.\n",
    "            # TODO See for impossible moves\n",
    "            \n",
    "            # sample random minibatch from self.R\n",
    "            batch = random.sample(self.R, self.batch_size)\n",
    "\n",
    "            # convert to tensor\n",
    "            states = torch.cat([x[0] for x in batch]).to(self.device)\n",
    "            actions = [(i,x[1]) for i,x in enumerate(batch)]\n",
    "            rewards = torch.tensor([x[2] for x in batch]).to(self.device)\n",
    "            s_primes = torch.cat([x[3] for x in batch if x[3] is not None]).to(self.device)\n",
    "            s_prime_mask = torch.tensor([x[3] is not None for x in batch], device=self.device, dtype=torch.bool)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            Q_theta_sj_aj = self.forward(states)[actions]\n",
    "            maxQtarget = torch.zeros(self.batch_size, device=self.device)\n",
    "            maxQtarget[s_prime_mask] = self.Qtarget.forward(s_primes).max(dim=1).values.detach()\n",
    "\n",
    "            loss = self.criterion(Q_theta_sj_aj, rewards + self.discountFactor*maxQtarget)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.t += 1\n",
    "            if self.t == self.C:\n",
    "                self.t = 0\n",
    "                self.Qtarget.load_state_dict(self.state_dict())\n",
    "        \n",
    "        elif end:\n",
    "            self.state = None\n",
    "            self.action = None\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
