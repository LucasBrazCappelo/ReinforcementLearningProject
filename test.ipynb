{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "\n",
    "import multiprocessing as mp\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork(nn.Module):\n",
    "    def __init__(self, input_size=18, hidden_size1=128, hidden_size2=128, output_size=9):\n",
    "        super(Qnetwork, self).__init__()\n",
    "        self.flattener = nn.Flatten()\n",
    "        self.inputLayer = nn.Linear(input_size, hidden_size1)\n",
    "        self.fullyConnected = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.outputLayer = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flattener(x)\n",
    "        x = F.relu(self.inputLayer(x))\n",
    "        x = F.relu(self.fullyConnected(x))\n",
    "        x = self.outputLayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,epsilon=0.2,player='X', learningRate=0.05, discountFactor=0.99, n_max=100):\n",
    "        if isinstance(epsilon, tuple):\n",
    "            self.epsilon_min, self.epsilon_max = epsilon\n",
    "            self.epsilon = self.epsilon_max\n",
    "        else:\n",
    "            self.epsilon = epsilon\n",
    "            self.epsilon_min = epsilon\n",
    "            self.epsilon_max = epsilon\n",
    "        self.learningRate = learningRate\n",
    "        self.discountFactor = discountFactor\n",
    "\n",
    "        self.state = None\n",
    "        self.action = None\n",
    "\n",
    "        self.n = 0\n",
    "        self.n_max = n_max\n",
    "\n",
    "        self.isLearning = True\n",
    "\n",
    "        self.player = player # 'X' or 'O'\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon_max * (1 - self.n / self.n_max))\n",
    "\n",
    "    def set_player(self, player = 'X', j=-1):\n",
    "        self.player = player\n",
    "        if j != -1:\n",
    "            self.player = 'X' if j % 2 == 0 else 'O'\n",
    "\n",
    "    def empty(self, state):\n",
    "        \"\"\" Return all empty positions. \"\"\"\n",
    "        availableActions = []\n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                position = (x, y)\n",
    "                if state[position] == 0:\n",
    "                    availableActions.append(position)\n",
    "        return availableActions\n",
    "\n",
    "    def randomAction(self, state):\n",
    "        \"\"\" Choose a random action from the available options. \"\"\"\n",
    "        availableActions = self.empty(state)\n",
    "\n",
    "        return random.choice(availableActions)\n",
    "    \n",
    "    def bestAction(self, state):\n",
    "        pass\n",
    "\n",
    "    def act(self,state):\n",
    "        pass\n",
    "\n",
    "    def learn(self, s_prime, reward, end=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent(Agent):\n",
    "    \"\"\"\n",
    "    Our Q-network will be a simple linear neural network with two hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=0.2, player='X', learningRate=0.0005, discountFactor=1.0 , n_max=100, Qmodel=Qnetwork(), batch_size=64, C=500 , R=deque(maxlen=10_000), criterion=nn.HuberLoss()):\n",
    "        super(DQN_agent, self).__init__()\n",
    "\n",
    "        # If a GPU is available\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.Qmodel = Qmodel.to(self.device)\n",
    "        self.Qtarget = Qnetwork().to(self.device)\n",
    "        self.Qtarget.load_state_dict(self.Qmodel.state_dict())\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.t = 0\n",
    "        self.C = C\n",
    "\n",
    "        self.R = R\n",
    "\n",
    "        # criterion is Huber loss (with delta = 1)\n",
    "        self.criterion = criterion\n",
    "\n",
    "        # optimizer is Adam\n",
    "        self.optimizer = torch.optim.Adam(self.Qmodel.parameters(), lr=learningRate)        \n",
    "\n",
    "    def bestAction(self, state):\n",
    "        \"\"\"\n",
    "        Choose the available actions which have a maximum expected future reward\n",
    "        using the Q-network.\n",
    "        \"\"\"\n",
    "        # convert state to tensor, adding batch dimension\n",
    "        with torch.no_grad():\n",
    "            q_values = self.Qmodel.forward(state)\n",
    "        return q_values.argmax(dim=1).item()\n",
    "\n",
    "    def act(self, grid):\n",
    "        \"\"\"\n",
    "        epsilon-greedy action selection, according to the Q-table.\n",
    "        \"\"\"\n",
    "        state = torch.tensor(grid, dtype=torch.int64)\n",
    "        state = F.one_hot(state+1,3)\n",
    "        state = state[:,:,(2,0)]\n",
    "        state = state.unsqueeze(0)\n",
    "        state = state.type(torch.float).to(self.device)\n",
    "        self.state = state\n",
    "\n",
    "        # whether move in random or not\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.randomAction(grid)\n",
    "            self.action = action[0] * 3 + action[1]\n",
    "        else:\n",
    "            # Get the best move\n",
    "            self.action = self.bestAction(self.state)\n",
    "            # action is a tuple of (x, y) from self.action\n",
    "            action = (self.action // 3, self.action % 3)\n",
    "\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self, grid, reward, end=False):\n",
    "        if self.isLearning:\n",
    "            if not end:\n",
    "                s_prime = torch.tensor(grid, dtype=torch.int64)\n",
    "                s_prime = F.one_hot(s_prime+1,3)\n",
    "                s_prime = s_prime[:,:,(2,0)]\n",
    "                s_prime = s_prime.unsqueeze(0)\n",
    "                s_prime = s_prime.type(torch.float).to(self.device)\n",
    "\n",
    "                self.R.append((self.state, self.action, reward, s_prime))\n",
    "            else:\n",
    "                self.R.append((self.state, self.action, reward, None))\n",
    "\n",
    "                self.state = None\n",
    "                self.action = None\n",
    "\n",
    "                self.n += 1\n",
    "                self.decrease_epsilon()\n",
    "            # self.R is a deque with maxlen=buffer_size so it auto pop\n",
    "\n",
    "            if len(self.R) < self.batch_size:\n",
    "                batch = self.R\n",
    "                maxQtarget = torch.zeros(len(self.R)).to(self.device)\n",
    "            else:\n",
    "                # sample random minibatch from self.R\n",
    "                batch = random.sample(self.R, self.batch_size)\n",
    "                maxQtarget = torch.zeros(self.batch_size).to(self.device) \n",
    "\n",
    "            # convert to tensor\n",
    "            states = torch.cat([x[0] for x in batch]).to(self.device)\n",
    "            actions = [x[1] for x in batch]\n",
    "            rewards = torch.tensor([x[2] for x in batch]).to(self.device)\n",
    "            s_primes = torch.cat([x[3] for x in batch if x[3] is not None]).to(self.device) \n",
    "            s_prime_mask = torch.tensor([x[3] is not None for x in batch], dtype=torch.bool).to(self.device)  \n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            Q_theta_sj_aj = self.Qmodel.forward(states)[torch.arange(len(actions)),actions]\n",
    "            \n",
    "            maxQtarget[s_prime_mask] = self.Qtarget.forward(s_primes).max(dim=1).values.detach()\n",
    "\n",
    "            loss = self.criterion(Q_theta_sj_aj, rewards + self.discountFactor*maxQtarget)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.t += 1\n",
    "            if self.t == self.C:\n",
    "                self.t = 0\n",
    "                self.Qtarget.load_state_dict(self.Qmodel.state_dict())\n",
    "        \n",
    "        elif end:\n",
    "            self.state = None\n",
    "            self.action = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_games(player_opt, agent, maxGames=20_000, env=TictactoeEnv()):\n",
    "    Turns = np.array(['X','O'])\n",
    "    winnerList = np.zeros(maxGames)\n",
    "\n",
    "    pBar = trange(maxGames)\n",
    "    for nbGames in range(maxGames):\n",
    "        env.reset()\n",
    "        grid, _, __ = env.observe()\n",
    "\n",
    "        player_opt.player = Turns[nbGames%2]\n",
    "        agent.player = Turns[(nbGames+1)%2]\n",
    "\n",
    "        for roundGame in range(9):\n",
    "            if env.current_player == player_opt.player:\n",
    "                if roundGame > 1 and isinstance(player_opt, Agent):\n",
    "                    player_opt.learn(grid, 0)\n",
    "                move = player_opt.act(grid)   \n",
    "                badMove = env.grid[move] != 0\n",
    "                if badMove:\n",
    "                    if not player_opt.isLearning:\n",
    "                        print(\"A player should continue to learn before playing for real.\")\n",
    "                        break\n",
    "                while badMove:\n",
    "                    player_opt.learn(grid, -1)\n",
    "                    move = player_opt.act(grid)\n",
    "                    badMove = env.grid[move] != 0\n",
    "            else:\n",
    "                if roundGame > 1 and isinstance(agent, Agent):\n",
    "                    agent.learn(grid, 0)\n",
    "                move = agent.act(grid)   \n",
    "                badMove = env.grid[move] != 0\n",
    "                if badMove:\n",
    "                    if not agent.isLearning:\n",
    "                        print(\"A player should continue to learn before playing for real.\")\n",
    "                        break\n",
    "                while badMove:\n",
    "                    agent.learn(grid, -1)\n",
    "                    move = agent.act(grid)\n",
    "                    badMove = env.grid[move] != 0\n",
    "                             \n",
    "\n",
    "            grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "            if end:\n",
    "                if winner == agent.player:\n",
    "                    winnerList[nbGames] = 1\n",
    "                    if isinstance(player_opt, Agent):\n",
    "                        player_opt.learn(grid, -1, end=True)\n",
    "                    if isinstance(agent, Agent):\n",
    "                        agent.learn(grid, 1, end=True)\n",
    "                elif winner == player_opt.player:\n",
    "                    winnerList[nbGames] = -1\n",
    "                    if isinstance(player_opt, Agent):\n",
    "                        player_opt.learn(grid, 1, end=True)\n",
    "                    if isinstance(agent, Agent):\n",
    "                        agent.learn(grid, -1, end=True)\n",
    "                else:\n",
    "                    if isinstance(player_opt, Agent):\n",
    "                        player_opt.learn(grid, 0, end=True)\n",
    "                    if isinstance(agent, Agent):\n",
    "                        agent.learn(grid, 0, end=True)\n",
    "                break     \n",
    "        pBar.update(1)\n",
    "    pBar.close()\n",
    "    env.reset()\n",
    "    return winnerList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905899e1b5a348598149a2dce73a0639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          250,
          500,
          750,
          1000,
          1250,
          1500,
          1750,
          2000,
          2250,
          2500,
          2750,
          3000,
          3250,
          3500,
          3750,
          4000,
          4250,
          4500,
          4750,
          5000,
          5250,
          5500,
          5750,
          6000,
          6250,
          6500,
          6750,
          7000,
          7250,
          7500,
          7750,
          8000,
          8250,
          8500,
          8750,
          9000,
          9250,
          9500,
          9750,
          10000,
          10250,
          10500,
          10750,
          11000,
          11250,
          11500,
          11750,
          12000,
          12250,
          12500,
          12750,
          13000,
          13250,
          13500,
          13750,
          14000,
          14250,
          14500,
          14750,
          15000,
          15250,
          15500,
          15750,
          16000,
          16250,
          16500,
          16750,
          17000,
          17250,
          17500,
          17750,
          18000,
          18250,
          18500,
          18750,
          19000,
          19250,
          19500,
          19750
         ],
         "xaxis": "x",
         "y": [
          -0.448,
          -0.232,
          -0.024,
          0.076,
          -0.024,
          0.144,
          0.144,
          0.18,
          0.18,
          0.072,
          0.208,
          0.14,
          0.136,
          0.172,
          0.216,
          0.276,
          0.16,
          0.184,
          0.256,
          0.292,
          0.18,
          0.3,
          0.368,
          0.304,
          0.176,
          0.396,
          0.232,
          0.272,
          0.2,
          0.356,
          0.356,
          0.276,
          0.376,
          0.316,
          0.34,
          0.292,
          0.272,
          0.352,
          0.2,
          0.292,
          0.288,
          0.36,
          0.324,
          0.244,
          0.324,
          0.276,
          0.308,
          0.296,
          0.256,
          0.268,
          0.22,
          0.32,
          0.276,
          0.276,
          0.308,
          0.288,
          0.264,
          0.192,
          0.196,
          0.396,
          0.368,
          0.228,
          0.324,
          0.288,
          0.284,
          0.312,
          0.316,
          0.22,
          0.324,
          0.344,
          0.356,
          0.436,
          0.276,
          0.316,
          0.24,
          0.288,
          0.224,
          0.248,
          0.364,
          0.368
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Average reward over time of RL agent with policy epsilon=0.6"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Game number"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Average reward"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon = 0.6\n",
    "\n",
    "player_opt = OptimalPlayer(epsilon=0.5)\n",
    "agent = DQN_agent(epsilon=epsilon)\n",
    "\n",
    "winnerList = play_games(player_opt, agent, maxGames=20_000)\n",
    "\n",
    "groupSize = 250\n",
    "y=winnerList.reshape(winnerList.size//groupSize, groupSize).mean(axis=1)\n",
    "x=np.arange(y.size)*groupSize\n",
    "\n",
    "fig = px.line(x=x, y=y, title=f'Average reward over time of RL agent with policy epsilon={epsilon}')\n",
    "fig.update_layout(xaxis_title='Game number', yaxis_title='Average reward')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
