{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BRAZ Lucas & DURAND Pierre-Alain\n",
    "## SCIPER: 343141 & SCIPER: 344313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "import torch.nn.functional as nn_functional\n",
    "from joblib import Parallel, delayed\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from src.utils import DQNAgent, OptimalPlayer, QlearningAgent, play_games, play_games_with_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(343141)\n",
    "np.random.seed(343141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a GPU is available\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception(\"Things will go much quicker if you use a GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 *Q*-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "player_opt = OptimalPlayer(epsilon=0.5)\n",
    "agent = QlearningAgent(epsilon=epsilon)\n",
    "\n",
    "winner_list = play_games(player_opt, agent, max_games=20_000)\n",
    "\n",
    "group_size = 250\n",
    "y = winner_list.reshape(winner_list.size // group_size, group_size).mean(axis=1)\n",
    "x = np.arange(y.size) * group_size\n",
    "\n",
    "fig = px.line(x=x, y=y, title=f\"Average reward over time of RL agent with policy epsilon={epsilon}\")\n",
    "fig.update_layout(width=1000, xaxis_title=\"Game number\", yaxis_title=\"Average reward\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Decreasing exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_list = [1, 100, 1_000, 10_000, 20_000, 30_000, 40_000]\n",
    "\n",
    "epsilon = (0.1, 0.8)\n",
    "\n",
    "max_games_total = 20_000\n",
    "group_size = 250\n",
    "df = {\"Game number\": np.arange(max_games_total // group_size) * group_size}\n",
    "\n",
    "player_opt = OptimalPlayer(epsilon=0.5)\n",
    "\n",
    "num_cores = min(len(n_max_list), mp.cpu_count())\n",
    "\n",
    "\n",
    "def parallel_games(n_max, player_opt_, epsilon_, max_games_total_, group_size_):\n",
    "    df_ = {}\n",
    "\n",
    "    agent_ = QlearningAgent(epsilon=epsilon_, n_max=n_max)\n",
    "\n",
    "    winner_list_ = play_games(player_opt_, agent_, max_games=max_games_total_)\n",
    "\n",
    "    y_ = winner_list_.reshape(winner_list_.size // group_size_, group_size_).mean(axis=1)\n",
    "\n",
    "    df_[f\"Average reward (n*={n_max})\"] = y_\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "dfs = Parallel(n_jobs=num_cores)(\n",
    "    delayed(parallel_games)(n_max, player_opt, epsilon, max_games_total, group_size)\n",
    "    for n_max in n_max_list\n",
    ")\n",
    "\n",
    "for d in dfs:\n",
    "    df.update(d)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Game number\",\n",
    "    y=df.columns.difference([\"Game number\"]),\n",
    "    title=f\"Average reward over time of RL agent with policy epsilon={epsilon}\",\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_list = [1, 100, 10_000, 40_000]\n",
    "\n",
    "epsilon = (0.1, 0.8)\n",
    "\n",
    "max_games_total = 20_000\n",
    "group_size = 250\n",
    "df = {\"Game number\": np.arange(max_games_total // group_size) * group_size}\n",
    "\n",
    "\n",
    "num_cores = min(len(n_max_list), mp.cpu_count())\n",
    "\n",
    "\n",
    "def parallel_games(n_max, epsilon_, max_games_total_, group_size_):\n",
    "    df_ = {}\n",
    "\n",
    "    player_opt_ = OptimalPlayer(epsilon=0.5)\n",
    "    agent_ = QlearningAgent(epsilon=epsilon_, n_max=n_max)\n",
    "\n",
    "    winner_list_, m_opt, m_random = play_games_with_m(\n",
    "        player_opt_, agent_, max_games_total_, group_size_\n",
    "    )\n",
    "\n",
    "    df_[f\"m_opt (n*={n_max})\"] = m_opt\n",
    "    df_[f\"m_random (n*={n_max})\"] = m_random\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "dfs = Parallel(n_jobs=num_cores)(\n",
    "    delayed(parallel_games)(n_max, epsilon, max_games_total, group_size) for n_max in n_max_list\n",
    ")\n",
    "\n",
    "for d in dfs:\n",
    "    df.update(d)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Game number\",\n",
    "    y=df.columns.difference([\"Game number\"]),\n",
    "    title=f\"Average reward over time of RL agent with policy epsilon={epsilon}\",\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Good experts and bad experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_opt_list = [0, 0.1, 0.7, 1.0]\n",
    "\n",
    "n_max = 1\n",
    "\n",
    "epsilon = (0.1, 0.8)\n",
    "\n",
    "max_games_total = 20_000\n",
    "group_size = 250\n",
    "df = {\"Game number\": np.arange(max_games_total // group_size) * group_size}\n",
    "\n",
    "num_cores = mp.cpu_count()\n",
    "\n",
    "\n",
    "def parallel_games(epsilon_opt, max_games_total_, group_size_, epsilon_, n_max):\n",
    "    df_ = {}\n",
    "\n",
    "    agent_ = QlearningAgent(epsilon=epsilon_, n_max=n_max)\n",
    "    player_opt_ = OptimalPlayer(epsilon=epsilon_opt)\n",
    "\n",
    "    _, m_opt, m_random = play_games_with_m(player_opt_, agent_, max_games_total_, group_size_)\n",
    "\n",
    "    # y = winner_list.reshape(winner_list.size//group_size_, group_size_).mean(axis=1)\n",
    "\n",
    "    # df_[f'Average reward (epsilon_opt={epsilon_opt})'] = y\n",
    "    df_[f\"m_opt (epsilon_opt={epsilon_opt})\"] = m_opt\n",
    "    df_[f\"m_random (epsilon_opt={epsilon_opt})\"] = m_random\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "dfs = Parallel(n_jobs=num_cores)(\n",
    "    delayed(parallel_games)(epsilon_opt, max_games_total, group_size, epsilon, n_max)\n",
    "    for epsilon_opt in epsilon_opt_list\n",
    ")\n",
    "\n",
    "for d in dfs:\n",
    "    df.update(d)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Game number\",\n",
    "    y=df.columns.difference([\"Game number\"]),\n",
    "    title=f\"Average reward over time of RL agent with policy epsilon={epsilon}\",\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Learning by self-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "n_max = 1\n",
    "\n",
    "max_games_total = 20_000\n",
    "group_size = 250\n",
    "df = {\"Game number\": np.arange(max_games_total // group_size) * group_size}\n",
    "\n",
    "num_cores = mp.cpu_count()\n",
    "\n",
    "\n",
    "def parallel_games(epsilon_, max_games_total_, group_size_, n_max):\n",
    "    df_ = {}\n",
    "\n",
    "    agent_ = QlearningAgent(epsilon=epsilon_, n_max=n_max)\n",
    "    player_opt_ = QlearningAgent(\n",
    "        epsilon=agent_.epsilon,\n",
    "        learning_rate=agent_.learning_rate,\n",
    "        discount_factor=agent_.discount_factor,\n",
    "        n_max=agent_.n_max,\n",
    "        q=agent_.q,\n",
    "    )\n",
    "\n",
    "    _, m_opt, m_random = play_games_with_m(player_opt_, agent_, max_games_total_, group_size_)\n",
    "\n",
    "    df_[f\"m_opt (epsilon={epsilon_})\"] = m_opt\n",
    "    df_[f\"m_random (epsilon={epsilon_})\"] = m_random\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "dfs = Parallel(n_jobs=num_cores)(\n",
    "    delayed(parallel_games)(epsilon, max_games_total, group_size, n_max) for epsilon in epsilon_list\n",
    ")\n",
    "\n",
    "for d in dfs:\n",
    "    df.update(d)\n",
    "\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Game number\",\n",
    "    y=df.columns.difference([\"Game number\"]),\n",
    "    title=\"Average reward over time of RL agent against himself\",\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_list = [1, 1_000, 10_000, 40_000]\n",
    "\n",
    "epsilon = (0.1, 0.8)\n",
    "\n",
    "max_games_total = 20_000\n",
    "group_size = 250\n",
    "df = {\"Game number\": np.arange(max_games_total // group_size) * group_size}\n",
    "\n",
    "num_cores = min(len(n_max_list), mp.cpu_count())\n",
    "\n",
    "\n",
    "def parallel_games(n_max, epsilon_, max_games_total_, group_size_):\n",
    "    df_ = {}\n",
    "\n",
    "    agent_ = QlearningAgent(epsilon=epsilon_, n_max=n_max)\n",
    "    player_opt_ = QlearningAgent(\n",
    "        epsilon=agent_.epsilon,\n",
    "        learning_rate=agent_.learning_rate,\n",
    "        discount_factor=agent_.discount_factor,\n",
    "        n_max=agent_.n_max,\n",
    "        q=agent_.q,\n",
    "    )\n",
    "\n",
    "    _, m_opt, m_random = play_games_with_m(player_opt_, agent_, max_games_total_, group_size_)\n",
    "\n",
    "    # y = winner_list.reshape(winner_list.size//group_size_, group_size_).mean(axis=1)\n",
    "\n",
    "    # df_[f'Average reward (n*={n_max})'] = y\n",
    "    df_[f\"m_opt (n*={n_max})\"] = m_opt\n",
    "    df_[f\"m_random (n*={n_max})\"] = m_random\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "dfs = Parallel(n_jobs=num_cores)(\n",
    "    delayed(parallel_games)(n_max, epsilon, max_games_total, group_size) for n_max in n_max_list\n",
    ")\n",
    "\n",
    "for d in dfs:\n",
    "    df.update(d)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Game number\",\n",
    "    y=df.columns.difference([\"Game number\"]),\n",
    "    title=f\"Average reward over time of RL agent with policy epsilon={epsilon}\",\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=(\"initial state\", \"State of highest Q-value\", \"State of lowest Q-value\"),\n",
    ")\n",
    "\n",
    "state = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "state_img = state.copy().astype(float).astype(str)\n",
    "state_img[state_img == \"1.0\"] = \"X\"\n",
    "state_img[state_img == \"-1.0\"] = \"O\"\n",
    "state_img[state_img == \"0.0\"] = \"-\"\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=[\"0\", \"1\", \"2\"],\n",
    "        y=[\"0\", \"1\", \"2\"],\n",
    "        z=[\n",
    "            [agent.q[state, (0, 0)], agent.q[state, (0, 1)], agent.q[state, (0, 2)]],\n",
    "            [agent.q[state, (1, 0)], agent.q[state, (1, 1)], agent.q[state, (1, 2)]],\n",
    "            [agent.q[state, (2, 0)], agent.q[state, (2, 1)], agent.q[state, (2, 2)]],\n",
    "        ],\n",
    "        text=state_img,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 20},\n",
    "        colorscale=\"gray\",\n",
    "        zmin=min(agent.q.q_tab.values()),\n",
    "        zmax=max(agent.q.q_tab.values()),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "state = agent.q.reverse_hash(max(agent.q.q_tab, key=agent.q.q_tab.get))[0]\n",
    "state_img = state.copy().astype(float).astype(str)\n",
    "state_img[state_img == \"1.0\"] = \"X\"\n",
    "state_img[state_img == \"-1.0\"] = \"O\"\n",
    "state_img[state_img == \"0.0\"] = \"-\"\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=[\"0\", \"1\", \"2\"],\n",
    "        y=[\"0\", \"1\", \"2\"],\n",
    "        z=[\n",
    "            [agent.q[state, (0, 0)], agent.q[state, (0, 1)], agent.q[state, (0, 2)]],\n",
    "            [agent.q[state, (1, 0)], agent.q[state, (1, 1)], agent.q[state, (1, 2)]],\n",
    "            [agent.q[state, (2, 0)], agent.q[state, (2, 1)], agent.q[state, (2, 2)]],\n",
    "        ],\n",
    "        text=state_img,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 20},\n",
    "        colorscale=\"gray\",\n",
    "        zmin=min(agent.q.q_tab.values()),\n",
    "        zmax=max(agent.q.q_tab.values()),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "state = agent.q.reverse_hash(min(agent.q.q_tab, key=agent.q.q_tab.get))[0]\n",
    "state_img = state.copy().astype(float).astype(str)\n",
    "state_img[state_img == \"1.0\"] = \"X\"\n",
    "state_img[state_img == \"-1.0\"] = \"O\"\n",
    "state_img[state_img == \"0.0\"] = \"-\"\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=[\"0\", \"1\", \"2\"],\n",
    "        y=[\"0\", \"1\", \"2\"],\n",
    "        z=[\n",
    "            [agent.q[state, (0, 0)], agent.q[state, (0, 1)], agent.q[state, (0, 2)]],\n",
    "            [agent.q[state, (1, 0)], agent.q[state, (1, 1)], agent.q[state, (1, 2)]],\n",
    "            [agent.q[state, (2, 0)], agent.q[state, (2, 1)], agent.q[state, (2, 2)]],\n",
    "        ],\n",
    "        text=state_img,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 20},\n",
    "        colorscale=\"gray\",\n",
    "        zmin=min(agent.q.q_tab.values()),\n",
    "        zmax=max(agent.q.q_tab.values()),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=3,\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.update_layout(width=1300, height=500, title=\"Different Q-values for different states\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Deep *Q*-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "player_opt = OptimalPlayer(epsilon=0.5)\n",
    "agent = DQNAgent(epsilon=epsilon)\n",
    "\n",
    "winner_list = play_games(player_opt, agent, max_games=20_000)\n",
    "\n",
    "group_size = 250\n",
    "y = winner_list.reshape(winner_list.size // group_size, group_size).mean(axis=1)\n",
    "x = np.arange(y.size) * group_size\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    subplot_titles=(\n",
    "        f\"Average reward over time of DQN agent with policy epsilon={epsilon}\",\n",
    "        f\"Loss over time of DQN agent with policy epsilon={epsilon}\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=y), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(agent.loss_curve)), y=agent.loss_curve), row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=700, width=1000)\n",
    "fig.update_xaxes(title_text=\"Game number\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Game number\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Average reward\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Loss\", row=2, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "player_opt = OptimalPlayer(epsilon=0.5)\n",
    "agent = DQNAgent(epsilon=epsilon, batch_size=1, r=deque(maxlen=1))\n",
    "\n",
    "winner_list = play_games(player_opt, agent, max_games=20_000)\n",
    "\n",
    "group_size = 250\n",
    "y = winner_list.reshape(winner_list.size // group_size, group_size).mean(axis=1)\n",
    "x = np.arange(y.size) * group_size\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    subplot_titles=(\n",
    "        f\"Average reward over time of DQN agent with policy epsilon={epsilon}\",\n",
    "        f\"Loss over time of DQN agent with policy epsilon={epsilon}\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=y), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(agent.loss_curve)), y=agent.loss_curve), row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=700, width=1000)\n",
    "fig.update_xaxes(title_text=\"Game number\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Game number\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Average reward\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Loss\", row=2, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_list = [1, 1_000, 10_000, 40_000]\n",
    "\n",
    "epsilon = (0.1, 0.8)\n",
    "\n",
    "max_games_total = 20_000\n",
    "group_size = 250\n",
    "df = {\"Game number\": np.arange(max_games_total // group_size) * group_size}\n",
    "\n",
    "\n",
    "num_cores = min(len(n_max_list), mp.cpu_count())\n",
    "\n",
    "\n",
    "def parallel_games(n_max, epsilon_, max_games_total_, group_size_):\n",
    "    df_ = {}\n",
    "\n",
    "    player_opt_ = OptimalPlayer(epsilon=0.5)\n",
    "    agent_ = DQNAgent(epsilon=epsilon_, n_max=n_max)\n",
    "\n",
    "    _, m_opt, m_random = play_games_with_m(player_opt_, agent_, max_games_total_, group_size_)\n",
    "\n",
    "    # y = winner_list.reshape(winner_list.size//group_size_, group_size_).mean(axis=1)\n",
    "\n",
    "    # df_[f'Average reward (n*={n_max})'] = y\n",
    "    df_[f\"m_opt (n*={n_max})\"] = m_opt\n",
    "    df_[f\"m_random (n*={n_max})\"] = m_random\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "dfs = Parallel(n_jobs=2)(\n",
    "    delayed(parallel_games)(n_max, epsilon, max_games_total, group_size) for n_max in n_max_list\n",
    ")\n",
    "\n",
    "for d in dfs:\n",
    "    df.update(d)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Game number\",\n",
    "    y=df.columns.difference([\"Game number\"]),\n",
    "    title=f\"Average reward over time of DQN agent with policy epsilon={epsilon}\",\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_opt_list = [0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "n_max = 10\n",
    "\n",
    "epsilon = (0.1, 0.8)\n",
    "\n",
    "max_games_total = 20_000\n",
    "group_size = 250\n",
    "df = {\"Game number\": np.arange(max_games_total // group_size) * group_size}\n",
    "\n",
    "num_cores = mp.cpu_count()\n",
    "\n",
    "\n",
    "def parallel_games(epsilon_opt, max_games_total_, group_size_, epsilon_, n_max):\n",
    "    df_ = {}\n",
    "\n",
    "    agent_ = DQNAgent(epsilon=epsilon_, n_max=n_max)\n",
    "    player_opt_ = OptimalPlayer(epsilon=epsilon_opt)\n",
    "\n",
    "    _, m_opt, m_random = play_games_with_m(player_opt_, agent_, max_games_total_, group_size_)\n",
    "\n",
    "    # y = winner_list.reshape(winner_list.size//group_size_, group_size_).mean(axis=1)\n",
    "\n",
    "    # df_[f'Average reward (epsilon_opt={epsilon_opt})'] = y\n",
    "    df_[f\"m_opt (epsilon_opt={epsilon_opt})\"] = m_opt\n",
    "    df_[f\"m_random (epsilon_opt={epsilon_opt})\"] = m_random\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "dfs = Parallel(n_jobs=2)(\n",
    "    delayed(parallel_games)(epsilon_opt, max_games_total, group_size, epsilon, n_max)\n",
    "    for epsilon_opt in epsilon_opt_list\n",
    ")\n",
    "\n",
    "for d in dfs:\n",
    "    df.update(d)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Game number\",\n",
    "    y=df.columns.difference([\"Game number\"]),\n",
    "    title=f\"Average reward over time of DQN agent with policy epsilon={epsilon}\",\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Learning by self-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_list = [0, 0.1, 0.6, 1.0]\n",
    "\n",
    "max_games_total = 20_000\n",
    "group_size = 250\n",
    "df = {\"Game number\": np.arange(max_games_total // group_size) * group_size}\n",
    "\n",
    "\n",
    "def parallel_games(epsilon_, max_games_total_, group_size_):\n",
    "    df_ = {}\n",
    "\n",
    "    player_opt_ = DQNAgent(epsilon=epsilon_, second_player=True)\n",
    "    agent_ = DQNAgent(epsilon=epsilon_, r=player_opt_.r, q_model=player_opt_.q_model)\n",
    "\n",
    "    _, m_opt, m_random = play_games_with_m(player_opt_, agent_, max_games_total_, group_size_)\n",
    "\n",
    "    df_[f\"m_opt (epsilon={epsilon_})\"] = m_opt\n",
    "    df_[f\"m_random (epsilon={epsilon_})\"] = m_random\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "dfs = Parallel(n_jobs=2)(\n",
    "    delayed(parallel_games)(epsilon, max_games_total, group_size) for epsilon in epsilon_list\n",
    ")\n",
    "\n",
    "for d in dfs:\n",
    "    df.update(d)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Game number\",\n",
    "    y=df.columns.difference([\"Game number\"]),\n",
    "    title=\"Average reward over time of DQN agent against himself\",\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max_list = [1, 1000, 10_000, 40_000]\n",
    "epsilon = (0.1, 0.8)\n",
    "\n",
    "max_games_total = 20_000\n",
    "group_size = 250\n",
    "df = {\"Game number\": np.arange(max_games_total // group_size) * group_size}\n",
    "\n",
    "\n",
    "def parallel_games(epsilon_, n_max, max_games_total_, group_size_):\n",
    "    df_ = {}\n",
    "\n",
    "    player_opt_ = DQNAgent(epsilon=epsilon_, n_max=n_max, second_player=True)\n",
    "    agent_ = DQNAgent(epsilon=epsilon_, r=player_opt_.r, q_model=player_opt_.q_model, n_max=n_max)\n",
    "\n",
    "    _, m_opt, m_random = play_games_with_m(player_opt_, agent_, max_games_total_, group_size_)\n",
    "\n",
    "    df_[f\"m_opt (n*={n_max})\"] = m_opt\n",
    "    df_[f\"m_random (n*={n_max})\"] = m_random\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "dfs = Parallel(n_jobs=2)(\n",
    "    delayed(parallel_games)(epsilon, n_max, max_games_total, group_size) for n_max in n_max_list\n",
    ")\n",
    "\n",
    "for d in dfs:\n",
    "    df.update(d)\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "fig = px.line(\n",
    "    df,\n",
    "    x=\"Game number\",\n",
    "    y=df.columns.difference([\"Game number\"]),\n",
    "    title=\"Average reward over time of DQN agent against himself\",\n",
    ")\n",
    "fig.update_layout(width=1000)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=3,\n",
    "    subplot_titles=(\"initial state\", \"State of highest Q-value\", \"State of lowest Q-value\"),\n",
    ")\n",
    "\n",
    "state = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])\n",
    "state_img = state.copy().astype(float).astype(str)\n",
    "state_img[state_img == \"1.0\"] = \"X\"\n",
    "state_img[state_img == \"-1.0\"] = \"O\"\n",
    "state_img[state_img == \"0.0\"] = \"-\"\n",
    "\n",
    "state = torch.tensor(state, dtype=torch.int64)\n",
    "state = nn_functional.one_hot(state + 1, 3)\n",
    "state = state[:, :, (2, 0)]\n",
    "state = state.unsqueeze(0)\n",
    "state = state.type(torch.float).to(agent.device)\n",
    "with torch.no_grad():\n",
    "    state = agent.q_model.forward(state).detach().cpu().numpy()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=[\"0\", \"1\", \"2\"],\n",
    "        y=[\"0\", \"1\", \"2\"],\n",
    "        z=[\n",
    "            [state[0][0], state[0][1], state[0][2]],\n",
    "            [state[0][3], state[0][4], state[0][5]],\n",
    "            [state[0][6], state[0][7], state[0][8]],\n",
    "        ],\n",
    "        text=state_img,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 20},\n",
    "        colorscale=\"gray\",\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "state = np.array([[0, 0, 0], [1, -1, -1], [1, 0, 0]])\n",
    "state_img = state.copy().astype(float).astype(str)\n",
    "state_img[state_img == \"1.0\"] = \"X\"\n",
    "state_img[state_img == \"-1.0\"] = \"O\"\n",
    "state_img[state_img == \"0.0\"] = \"-\"\n",
    "\n",
    "state = torch.tensor(state, dtype=torch.int64)\n",
    "state = nn_functional.one_hot(state + 1, 3)\n",
    "state = state[:, :, (2, 0)]\n",
    "state = state.unsqueeze(0)\n",
    "state = state.type(torch.float).to(agent.device)\n",
    "with torch.no_grad():\n",
    "    state = agent.q_model.forward(state).detach().cpu().numpy()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=[\"0\", \"1\", \"2\"],\n",
    "        y=[\"0\", \"1\", \"2\"],\n",
    "        z=[\n",
    "            [state[0][0], state[0][1], state[0][2]],\n",
    "            [state[0][3], state[0][4], state[0][5]],\n",
    "            [state[0][6], state[0][7], state[0][8]],\n",
    "        ],\n",
    "        text=state_img,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 20},\n",
    "        colorscale=\"gray\",\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "state = np.array([[-1, 0, 0], [1, -1, 0], [1, -1, 1]])\n",
    "state_img = state.copy().astype(float).astype(str)\n",
    "state_img[state_img == \"1.0\"] = \"X\"\n",
    "state_img[state_img == \"-1.0\"] = \"O\"\n",
    "state_img[state_img == \"0.0\"] = \"-\"\n",
    "\n",
    "state = torch.tensor(state, dtype=torch.int64)\n",
    "state = nn_functional.one_hot(state + 1, 3)\n",
    "state = state[:, :, (2, 0)]\n",
    "state = state.unsqueeze(0)\n",
    "state = state.type(torch.float).to(agent.device)\n",
    "with torch.no_grad():\n",
    "    state = agent.q_model.forward(state).detach().cpu().numpy()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        x=[\"0\", \"1\", \"2\"],\n",
    "        y=[\"0\", \"1\", \"2\"],\n",
    "        z=[\n",
    "            [state[0][0], state[0][1], state[0][2]],\n",
    "            [state[0][3], state[0][4], state[0][5]],\n",
    "            [state[0][6], state[0][7], state[0][8]],\n",
    "        ],\n",
    "        text=state_img,\n",
    "        texttemplate=\"%{text}\",\n",
    "        textfont={\"size\": 20},\n",
    "        colorscale=\"gray\",\n",
    "        zmin=0,\n",
    "        zmax=1,\n",
    "    ),\n",
    "    row=1,\n",
    "    col=3,\n",
    ")\n",
    "\n",
    "fig.update_layout(width=1300, height=500, title=\"Different Q-values for different states\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
